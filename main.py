from fastapi import FastAPI, HTTPException, Body, Query
from fastapi.responses import HTMLResponse, JSONResponse
from typing import List, Optional
from pydantic import BaseModel, Field
import asyncio

# local imports
import scraper
import cookie_manager

app = FastAPI(
    title="High Performance Playwright Scraping Server",
    description="é«˜æ€§èƒ½ãªå‹•çš„ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒãƒ¼ã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å†åˆ©ç”¨ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€é©åŒ–ã€‚"
)

class CookieModel(BaseModel):
    name: str
    value: str
    domain: str
    path: str
    expires: float
    httpOnly: bool
    secure: bool
    sameSite: str = Field(..., pattern="^(Strict|Lax|None)$")

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
@app.on_event("shutdown")
async def shutdown_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«ãƒ–ãƒ©ã‚¦ã‚¶ãƒªã‚½ãƒ¼ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    await scraper._browser_pool.cleanup()

@app.get("/", response_class=HTMLResponse, summary="ã‚µãƒ¼ãƒãƒ¼çŠ¶æ…‹ç¢ºèª")
async def read_root():
    """ã‚µãƒ¼ãƒãƒ¼ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¾ã™ã€‚"""
    return "<h1>High Performance Scraping Server is running ğŸš€</h1>"

@app.get("/scrape", response_class=HTMLResponse, summary="URLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆæ¨™æº–é€Ÿåº¦ï¼‰")
async def scrape_url(
    url: str, 
    wait: Optional[float] = Query(2.0, ge=0.5, le=10.0, description="å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰")
):
    """
    æŒ‡å®šã•ã‚ŒãŸURLã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€æŒ‡å®šç§’æ•°å¾Œã®HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿”ã—ã¾ã™ã€‚
    ãƒ–ãƒ©ã‚¦ã‚¶ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å†åˆ©ç”¨ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã€‚
    """
    if not url.startswith(("http://", "https://")):
        raise HTTPException(
            status_code=400, 
            detail="ç„¡åŠ¹ãªURLå½¢å¼ã§ã™ã€‚http:// ã¾ãŸã¯ https:// ã§å§‹ã¾ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
        )
    
    try:
        html_content = await scraper.get_html_after_wait(url, wait)
        return HTMLResponse(content=html_content)
    except Exception as e:
        print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"å†…éƒ¨ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        )

@app.get("/scrape/quick", response_class=HTMLResponse, summary="é«˜é€Ÿã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
async def quick_scrape_url(url: str):
    """
    æŒ‡å®šã•ã‚ŒãŸURLã«é«˜é€Ÿã‚¢ã‚¯ã‚»ã‚¹ã—ã€1ç§’å¾Œã®HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿”ã—ã¾ã™ã€‚
    è»½é‡ãªå‡¦ç†ã§ã‚ˆã‚Šé«˜é€Ÿãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æä¾›ã€‚
    """
    if not url.startswith(("http://", "https://")):
        raise HTTPException(
            status_code=400, 
            detail="ç„¡åŠ¹ãªURLå½¢å¼ã§ã™ã€‚http:// ã¾ãŸã¯ https:// ã§å§‹ã¾ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
        )
    
    try:
        html_content = await scraper.quick_scrape(url)
        return HTMLResponse(content=html_content)
    except Exception as e:
        print(f"é«˜é€Ÿã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"å†…éƒ¨ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        )

# è¤‡æ•°URLåŒæ™‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
@app.post("/scrape/batch", summary="ãƒãƒƒãƒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
async def batch_scrape(
    urls: List[str] = Body(..., description="ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡URLã®ãƒªã‚¹ãƒˆ"),
    wait: Optional[float] = Body(2.0, ge=0.5, le=10.0, description="å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰")
):
    """
    è¤‡æ•°ã®URLã‚’ä¸¦åˆ—ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¾ã™ã€‚
    """
    # URLå½¢å¼ãƒã‚§ãƒƒã‚¯
    for url in urls:
        if not url.startswith(("http://", "https://")):
            raise HTTPException(
                status_code=400,
                detail=f"ç„¡åŠ¹ãªURLå½¢å¼ã§ã™: {url}"
            )
    
    if len(urls) > 10:
        raise HTTPException(
            status_code=400,
            detail="ä¸€åº¦ã«å‡¦ç†ã§ãã‚‹URLã¯10å€‹ã¾ã§ã§ã™ã€‚"
        )
    
    try:
        # ä¸¦åˆ—å®Ÿè¡Œ
        tasks = [scraper.get_html_after_wait(url, wait) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        response_data = []
        for i, (url, result) in enumerate(zip(urls, results)):
            if isinstance(result, Exception):
                response_data.append({
                    "url": url,
                    "success": False,
                    "error": str(result),
                    "html": None
                })
            else:
                response_data.append({
                    "url": url,
                    "success": True,
                    "error": None,
                    "html": result
                })
        
        return JSONResponse(content={"results": response_data})
        
    except Exception as e:
        print(f"ãƒãƒƒãƒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"å†…éƒ¨ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        )

# --- Cookieç®¡ç†ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ---

@app.get("/cookies", response_model=List[CookieModel], summary="å…¨Cookieã‚’å–å¾—")
async def get_all_cookies():
    """ã‚µãƒ¼ãƒãƒ¼ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã™ã¹ã¦ã®Cookieã‚’å–å¾—ã—ã¾ã™ã€‚"""
    return cookie_manager.load_cookies()

@app.post("/cookies", summary="Cookieã‚’è¿½åŠ ãƒ»æ›´æ–°")
async def add_or_update_user_cookies(cookies: List[CookieModel] = Body(...)):
    """
    æ–°ã—ã„Cookieã‚’è¿½åŠ ã™ã‚‹ã‹ã€æ—¢å­˜ã®Cookieã‚’åå‰ã§ä¸Šæ›¸ãã—ã¾ã™ã€‚
    """
    try:
        cookies_dict_list = [cookie.dict() for cookie in cookies]
        cookie_manager.add_or_update_cookies(cookies_dict_list)
        return JSONResponse(
            content={"status": "success", "message": "CookieãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸã€‚"}
        )
    except Exception as e:
        print(f"Cookieæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail="Cookieã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

@app.delete("/cookies", summary="Cookieã‚’å‰Šé™¤")
async def delete_user_cookies(
    names: List[str] = Query(..., description="å‰Šé™¤ã—ãŸã„Cookieã®åå‰ã®ãƒªã‚¹ãƒˆ")
):
    """æŒ‡å®šã•ã‚ŒãŸåå‰ã‚’æŒã¤Cookieã‚’å‰Šé™¤ã—ã¾ã™ã€‚"""
    if not names:
        raise HTTPException(
            status_code=400, 
            detail="å‰Šé™¤ã™ã‚‹Cookieã®åå‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        )
    
    try:
        cookie_manager.delete_cookies_by_name(names)
        return JSONResponse(
            content={"status": "success", "message": f"Cookie {names} ãŒå‰Šé™¤ã•ã‚Œã¾ã—ãŸã€‚"}
        )
    except Exception as e:
        print(f"Cookieå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail="Cookieã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/health", summary="ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯")
async def health_check():
    """ã‚µãƒ¼ãƒãƒ¼ã¨ãƒ–ãƒ©ã‚¦ã‚¶ãƒ—ãƒ¼ãƒ«ã®çŠ¶æ…‹ã‚’ç¢ºèªã—ã¾ã™ã€‚"""
    try:
        browser_active = scraper._browser_pool.browser is not None
        return JSONResponse(content={
            "status": "healthy",
            "browser_pool_active": browser_active,
            "timestamp": asyncio.get_event_loop().time()
        })
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"status": "unhealthy", "error": str(e)}
        )
